{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/open?id=1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh)\n",
    "Here it is of the format:\n",
    "word \\t label\\n\n",
    "for instance:\n",
    "postural\tB\n",
    "hypotension\tI\n",
    "\n",
    "here B-> Begin entity, I-> inside entity and O-> outside entity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lodha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lodha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all required libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "from spacy import displacy\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import MaxNLocator\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ayush', 'is', 'suffering', 'from', 'cold', 'and', 'fever']\n"
     ]
    }
   ],
   "source": [
    "sent=input(\"Enter the sentence\")\n",
    "tok_sent=nltk.word_tokenize(sent)\n",
    "print(tok_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ayush', 'is', 'suffering', 'from', 'cold', 'and', 'fever']\n",
      "['ayush', 'suffering', 'cold', 'fever']\n"
     ]
    }
   ],
   "source": [
    "sent2=[]\n",
    "act_sent=[]\n",
    "stop_words=stopwords.words('english')\n",
    "for word in tok_sent:\n",
    "    sent2.append(word.lower())\n",
    "print(sent2)\n",
    "for i in sent2:\n",
    "    if (i not in stop_words):\n",
    "        act_sent.append(i)\n",
    "print(act_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ayush suffering cold fever\n"
     ]
    }
   ],
   "source": [
    "fin_sent=\"\"\n",
    "for i in act_sent:\n",
    "    fin_sent=fin_sent+\" \"+i\n",
    "print(fin_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert tsv file to the format accepted by spaCy for training.\n",
    "One of the format supported by spaCy is:\n",
    "TRAIN_DATA = [[(Sentence, {entities: [(start, end, label)]], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_spacy(file_path):\n",
    "    ''' Converts data from:\n",
    "    word \\t label \\n word \\t label \\n \\n word \\t label\n",
    "    to: sentence, {entities : [(start, end, label), (stard, end, label)]}\n",
    "    '''\n",
    "    file = open(file_path, 'r')\n",
    "    training_data, entities, sentence, unique_labels = [], [], [], []\n",
    "    current_annotation = None\n",
    "    start =0\n",
    "    end = 0 # initialize counter to keep track of start and end characters\n",
    "    for line in file:\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        # lines with len > 1 are words\n",
    "        if len(line) > 1:\n",
    "            label = line[1]\n",
    "            if(label != 'O'):\n",
    "                label = line[1]+\"_Disease\"     # the .txt is formatted: label \\t word, label[0:2] = label_type\n",
    "            #label_type = line[0][0] # beginning of annotations - \"B\", intermediate - \"I\"\n",
    "            word = line[0]\n",
    "            sentence.append(word)\n",
    "            start = end\n",
    "            end += (len(word) + 1)  # length of the word + trailing space\n",
    "           \n",
    "            if label == 'I_Disease' :  # if at the end of an annotation\n",
    "                entities.append(( start,end-1, label))  # append the annotation\n",
    "                              \n",
    "            if label == 'B_Disease':                         # if beginning new annotation\n",
    "                entities.append(( start,end-1, label))# start annotation at beginning of word\n",
    "                \n",
    "           \n",
    "           \n",
    "            if label != 'O' and label not in unique_labels:\n",
    "                unique_labels.append(label)\n",
    " \n",
    "        # lines with len == 1 are breaks between sentences\n",
    "        if len(line) == 1:\n",
    "            if(len(entities) > 0):\n",
    "                sentence = \" \".join(sentence)\n",
    "                training_data.append([sentence, {'entities' : entities}])\n",
    "            # reset the counters and temporary lists\n",
    "            end = 0 \n",
    "            start = 0\n",
    "            entities, sentence = [], []\n",
    "            \n",
    "    file.close()\n",
    "    return training_data, unique_labels   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert our train data,test data and validation data to spaCy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, LABELS = load_data_spacy(r\"C:\\Users\\lodha\\Downloads\\CustomeNERspaCyv3.0-main (2)\\CustomeNERspaCyv3.0-main\\train.tsv\")\n",
    "print(TRAIN_DATA)\n",
    "print(len(TRAIN_DATA))\n",
    "TEST_DATA, _ = load_data_spacy(r\"C:\\Users\\lodha\\Downloads\\CustomeNERspaCyv3.0-main (2)\\CustomeNERspaCyv3.0-main\\test.tsv\")\n",
    "print(len(TEST_DATA))\n",
    "VALID_DATA, _ = load_data_spacy(r\"C:\\Users\\lodha\\Downloads\\CustomeNERspaCyv3.0-main (2)\\CustomeNERspaCyv3.0-main\\train_dev.tsv\")\n",
    "print(len(VALID_DATA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/67407433/using-spacy-3-0-to-convert-data-from-old-spacy-v2-format-to-the-brand-new-spacy  The below method is used to convert the train and validation data from old format to new format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2658/2658 [00:05<00:00, 514.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5385/5385 [00:09<00:00, 582.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\") # save the docbin object\n",
    "\n",
    "db = DocBin()\n",
    "for text, annot in tqdm(VALID_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./valid.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 10:43:35.114409: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-03-16 10:43:35.114493: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " !python -m spacy train config.cfg --verbose --output ./ner_demo/training/ --paths.train train.spacy --paths.dev valid.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our model on  test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = spacy.load(R\"ner_demo/training/model-best\") #load the best model\n",
    "\n",
    "\n",
    "\n",
    "test_sentences = [x[0] for x in TEST_DATA[0:4000]] # extract the sentences from [sentence, entity]\n",
    "for x in test_sentences:\n",
    "    doc = ner(x)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    displacy.render(doc,jupyter=True, style = \"ent\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold : B_Disease\n",
      "fever : I_Disease\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ayush suffering \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cold\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B_Disease</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    fever\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I_Disease</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "#fin_sent = \"ayush suffering cold fever\"\n",
    "ner = spacy.load(R\"ner_demo/training/model-best\") #load the best model\n",
    "doc = ner(fin_sent)\n",
    "result = \"\"\n",
    "for ent in doc.ents:\n",
    "    texts = ent.text\n",
    "    label = ent.label_\n",
    "    result = result + texts + \" : \" + label + \"\\n\"\n",
    "print(result)\n",
    "displacy.render(doc,jupyter=True, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
